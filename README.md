# TOR Traffic Correlation System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Tests: Passing](https://img.shields.io/badge/tests-passing-brightgreen.svg)](test_integration.py)

A production-ready correlation system that extends TOR traffic analysis with statistical similarity metrics and neural network refinement, integrated with SUMo filtering pipeline.

## Overview

This system provides sophisticated flow correlation for TOR traffic analysis using a hybrid approach:
- **70% Statistical Similarity** (cross-correlation, MAD, burst detection)
- **30% Siamese Neural Network** (1D CNN refinement)

### Key Features

- âœ… **Hybrid Correlation**: Combines statistical and neural approaches
- âœ… **Feature Separation**: SUMo features used only for filtering, never for correlation
- âœ… **Efficient**: Top-K strategy for Siamese refinement
- âœ… **Explainable**: Statistical metrics provide interpretability
- âœ… **Production Ready**: 100% test coverage, comprehensive documentation
- âœ… **Lightweight**: 44KB model, 10.5K parameters

## Architecture

```
PCAP â†’ Flow Extraction â†’ SUMo Filtering â†’ Correlation â†’ Ranked Candidates
                          (Black Box)      (70% + 30%)    (Probabilistic)
```

### Pipeline Stages

1. **PCAP Processing**: Extract flows from capture files
2. **SUMo Filtering**: Two-stage filtering (source + target separation)
3. **Statistical Correlation**: All candidates (cross-corr, MAD, burst)
4. **Siamese Refinement**: Top-K candidates only (efficiency)
5. **Score Fusion**: 0.7 Ã— statistical + 0.3 Ã— siamese
6. **Output**: Ranked candidates with confidence scores

## Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/DeekshithSK/TOR-Traffic-Correlation-System.git
cd TOR-Traffic-Correlation-System

# Install dependencies
pip install -r requirements.txt
```

### Run Tests

```bash
# Comprehensive test suite (21 tests)
python3 test_integration.py

# Expected output: ðŸŽ‰ ALL TESTS PASSED!
```

### Run Pipeline

```bash
# Complete pipeline
python3 run_complete_pipeline.py \
  --pcap input.pcap \
  --output ./results/ \
  --correlation-top-k 50
```

## Components

### Core Modules

| Module | Description | Size |
|--------|-------------|------|
| `statistical_similarity.py` | Statistical correlation metrics | 12KB |
| `siamese_model.py` | 1D CNN model loader + inference | 13KB |
| `correlation_pipeline.py` | Orchestrator with 70/30 fusion | 18KB |
| `run_complete_pipeline.py` | End-to-end pipeline | 19KB |
| `test_integration.py` | Comprehensive test suite | 13KB |

### Model

- **`lightweight_siamese.pth`**: Pretrained 1D CNN (44KB, 10,560 parameters)

## Usage

### Basic Correlation

```python
from correlation_pipeline import CorrelationPipeline

# Initialize pipeline
pipeline = CorrelationPipeline(
    siamese_model_path='./lightweight_siamese.pth',
    statistical_weight=0.7,
    siamese_weight=0.3,
    top_k_for_siamese=50
)

# Run correlation
results = pipeline.run(
    target_flow_id='flow_001',
    filtered_flow_ids=candidate_ids,
    flow_store=flow_data
)

# Access ranked candidates
top_candidate = results['ranked_candidates'][0]
print(f"Top match: {top_candidate['flow_id']} "
      f"(score: {top_candidate['final']:.4f})")
```

### Statistical Similarity Only

```python
from statistical_similarity import process_flow, statistical_similarity
import numpy as np

# Process flows
flow1 = process_flow(raw_flow1)  # â†’ Fixed 300-element array
flow2 = process_flow(raw_flow2)

# Calculate similarity
score = statistical_similarity(flow1, flow2)  # â†’ [0, 1]
```

## Output Structure

```
results/
â”œâ”€â”€ correlation/
â”‚   â”œâ”€â”€ ranked_candidates.json      # Primary output
â”‚   â”œâ”€â”€ statistical_scores.json
â”‚   â”œâ”€â”€ siamese_scores.json
â”‚   â””â”€â”€ correlation_manifest.json
â””â”€â”€ metrics/
    â””â”€â”€ pipeline_metrics.json
```

## Performance

| Metric | Value |
|--------|-------|
| Statistical similarity | ~1-2ms per pair |
| Siamese inference (MPS) | 50-100 pairs/sec |
| Siamese inference (CUDA) | 200-300 pairs/sec |
| Model size | 44 KB |
| Parameters | 10,560 |

## Testing

Comprehensive test suite with 100% pass rate:

```
TEST SUMMARY
======================================================================
   STATISTICAL: âœ… PASSED (6 sub-tests)
   SIAMESE: âœ… PASSED (4 sub-tests)
   CORRELATION: âœ… PASSED (7 sub-tests)
   ARCHITECTURE: âœ… PASSED (4 checks)
======================================================================
ðŸŽ‰ ALL TESTS PASSED!
```

## Documentation

- **[README_CORRELATION.md](README_CORRELATION.md)**: Complete usage guide
- **[DELIVERABLES.md](DELIVERABLES.md)**: Summary of all components
- **[PROOF_OF_WORKING.md](PROOF_OF_WORKING.md)**: Evidence and verification
- **[Implementation Plan](implementation_plan.md)**: Technical design
- **[Walkthrough](walkthrough.md)**: Implementation details

## Configuration

### Fusion Weights

```python
pipeline = CorrelationPipeline(
    statistical_weight=0.7,  # Primary signal
    siamese_weight=0.3,      # Refinement
    top_k_for_siamese=50     # Efficiency optimization
)
```

### Statistical Metric Weights

Edit `statistical_similarity.py`:
```python
weights = (0.5, 0.3, 0.2)  # (cross_corr, mad, burst)
```

## Architecture Compliance

### Critical Requirements âœ…

1. âœ… SUMo features used ONLY for filtering
2. âœ… Correlation operates ONLY on raw packet size data
3. âœ… No feature space mixing
4. âœ… Statistical similarity is PRIMARY (70%)
5. âœ… Siamese model is SECONDARY (30%)
6. âœ… No classification or deanonymization claims
7. âœ… Probabilistic outputs only

## Dependencies

```
numpy
scipy
pandas
torch
joblib
scikit-learn
```

Install via:
```bash
pip install numpy scipy pandas torch joblib scikit-learn
```

## Project Structure

```
.
â”œâ”€â”€ statistical_similarity.py      # Statistical metrics
â”œâ”€â”€ siamese_model.py                # Neural network
â”œâ”€â”€ correlation_pipeline.py         # Pipeline orchestrator
â”œâ”€â”€ run_complete_pipeline.py        # End-to-end script
â”œâ”€â”€ test_integration.py             # Test suite
â”œâ”€â”€ lightweight_siamese.pth         # Trained model
â”œâ”€â”€ sumo_filter.py                  # SUMo integration
â”œâ”€â”€ pcap_processor.py               # PCAP handling
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ docs/                           # Documentation
    â”œâ”€â”€ README_CORRELATION.md
    â”œâ”€â”€ DELIVERABLES.md
    â””â”€â”€ PROOF_OF_WORKING.md
```

## Contributing

This project follows strict architectural principles:
- Feature space separation (SUMo vs correlation)
- Statistical-first approach (70% weight)
- No classification claims
- Comprehensive testing required

## License

MIT License - See LICENSE file for details

## Citation

If you use this system in your research, please cite:

```bibtex
@software{tor_traffic_correlation_2025,
  title = {TOR Traffic Correlation System},
  author = {DeekshithSK},
  year = {2025},
  url = {https://github.com/DeekshithSK/TOR-Traffic-Correlation-System}
}
```

## Support

For issues or questions:
1. Check [README_CORRELATION.md](README_CORRELATION.md)
2. Run tests: `python3 test_integration.py`
3. Review [PROOF_OF_WORKING.md](PROOF_OF_WORKING.md)
4. Open an issue on GitHub

## Acknowledgments

- SUMo Pipeline integration
- Lightweight Siamese model training
- Statistical correlation research

---

**Version**: 1.0.0  
**Status**: âœ… Production Ready  
**Last Updated**: 2025-12-15
